{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.c. Continuous reservoir simulations\n",
    "\n",
    "\n",
    "So far, we have utilized NSGA-II provided by iRONS to generate the Pareto front for reservoir release schedules (1.a. Initial reservoir release optimization). Subsequently, we have selected a compromising release schedule from this Pareto front using eight different Multi-Criteria Decision-Making (MCDM) techniques (1.b. Selection of compromise solution from the Pareto front).\n",
    "\n",
    "The aforementioned process needs to be conducted iteratively every month (for monthly decision-making) or every two months (for bimonthly decision-making) throughout the simulation period. The provided code facilitates this continuous simulation by utilizing the previously defined functions.\n",
    "\n",
    "\n",
    "\n",
    "![continuous_simulation](util/images/continuous_simulation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries\n",
    "\n",
    "We start to importing the necessary libraries and tools (üö® in order to run the code like in the box below, place the mouse pointer in the cell, then click on ‚Äúrun cell‚Äù button above or press shift + enter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from platypus import NSGAII, Problem, Real\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import calendar\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from irons.Software.read_data import read_csv_data\n",
    "from irons.Software.day2week2month import day2week\n",
    "from irons.Software.res_sys_sim import res_sys_sim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read functions for reservoir simulations\n",
    "\n",
    "Here, we read the functions that we defined from 1.a (release schedule optimisation) and 1.b (selection of compromise release schedule from the Pareto front) for continuous simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Functions for release optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reservoir_opt_deterministic(reservoir_name,month,year,leadtime,pop, itnum, scenario):\n",
    "    \n",
    "    input_path = path + '/data/' + scenario_list[scenario] + '/in/leadtime_' + str(leadtime) + '/' # folder containing the forecast weather data\n",
    "    observation_path = path + '/data/' + scenario_list[1] + '/in/leadtime_' + str(leadtime) + '/' # folder containing the forecast weather data    \n",
    "    output_path = path + '/data/' + scenario_list[scenario] + '/out/leadtime_' + str(leadtime) + '/' + MCDM_list[MCDM] + '/' # folder containing the forecast weather data\n",
    "\n",
    "    # Read potential evapotranspiration forecasts\n",
    "    e_fore_file = 'e_fore_' + str(year) + '_' + str(month) + '.csv'\n",
    "    dates_fore_day, e_fore_day = read_csv_data(input_path, e_fore_file)\n",
    "    dates_fore, e_fore_ens, e_fore_cum_ens = day2week(dates_fore_day, e_fore_day)\n",
    "    e_fore = np.zeros((dates_fore.shape[0],1))\n",
    "    e_fore[:,0] = e_fore_ens.mean(1)   # Average of the forecast ensemble\n",
    "    \n",
    "    # Read potential evapotranspiration observation\n",
    "    e_obs_file = 'e_fore_' + str(year) + '_' + str(month) + '.csv'\n",
    "    dates_obs_day, e_obs_day = read_csv_data(observation_path, e_obs_file)\n",
    "    dates_obs, e_obs_ens, e_obs_cum_ens = day2week(dates_obs_day,e_obs_day)\n",
    "    e_obs = np.zeros((dates_obs.shape[0],1))\n",
    "    e_obs[:,0] = e_obs_ens.mean(1)      # Average of the forecast ensemble\n",
    "\n",
    "    # Read inflow forecasts\n",
    "    I_fore_file = 'I_fore_'  + str(year) + '_' + str(month) + '.csv'\n",
    "    dates_fore_day, I_fore_day = read_csv_data(input_path, I_fore_file)\n",
    "    dates_fore, I_fore_ens, I_fore_cum_ens = day2week(dates_fore_day,I_fore_day)\n",
    "    I_fore = np.zeros((dates_fore.shape[0],1))\n",
    "    I_fore[:,0] = I_fore_ens.mean(1)      # Average of the forecast ensemble\n",
    "\n",
    "    # Read inflow observation\n",
    "    I_obs_file = 'I_fore_'  + str(year) + '_' + str(month) + '.csv'\n",
    "    dates_obs_day, I_obs_day = read_csv_data(observation_path, I_obs_file)\n",
    "    dates_obs, I_obs_ens, I_obs_cum_ens = day2week(dates_obs_day,I_obs_day)\n",
    "    I_obs = np.zeros((dates_obs.shape[0],1))\n",
    "    I_obs[:,0] = I_obs_ens.mean(1)         # Average of the forecast ensemble\n",
    "\n",
    "    # Read demand (fixed)\n",
    "    d_fore_file = 'd_fore_'  + str(year) + '_' + str(month) + '.csv'\n",
    "    dates_fore_day, d_fore_day = read_csv_data(input_path, d_fore_file)\n",
    "    dates_fore, d_fore_ens, d_fore_cum_ens = day2week(dates_fore_day,d_fore_day)\n",
    "    d_fore = np.zeros((dates_fore.shape[0],1))\n",
    "    d_fore[:,0] = d_fore_ens.mean(1)        # Average of the forecast ensemble\n",
    "    \n",
    "\n",
    "    # Read observation data and set initial storage\n",
    "    obsdata = pd.read_csv(path + '/data/observed_data.csv',index_col = 'Date')\n",
    "    obsdata.index = pd.to_datetime(obsdata.index)\n",
    "    \n",
    "    if year == start_year and month == start_month :\n",
    "        ini_date = dates_fore[0] # the initial date corresponds to the first date of the weekly forecast\n",
    "        s_ini = obsdata[obsdata.index == ini_date].iloc[0,0]    \n",
    "    else:\n",
    "        if scenario == 1:\n",
    "            s_ini = S_all['S_pf'][len(S_all)-1]\n",
    "        elif scenario == 2:\n",
    "            s_ini = S_all['S_wc'][len(S_all)-1]\n",
    "        elif scenario == 3:\n",
    "            s_ini = S_all['S_d20'][len(S_all) - 1]\n",
    "    \n",
    "    M = 1                                   # Number of Ensemble (if M=1, deterministic)\n",
    "    N = dates_fore.shape[0]                 # Number of forecast weeks\n",
    "    \n",
    "    # Define the target week for Storage Optimisation\n",
    "    if month + leadtime <10:\n",
    "        target_week = N\n",
    "    elif month >=10:\n",
    "        target_week = N\n",
    "    else:\n",
    "        target_week = N - 4*(month + leadtime - 10)\n",
    "  \n",
    "    # Release optimisation\n",
    "    pop_size = pop   # population size\n",
    "    num_iter = itnum # Number of iterations\n",
    "    \n",
    "    def auto_optim(vars):\n",
    "        Qreg_s_D = np.zeros((N,1))\n",
    "        # Decision vector: regulated release to meet the demand in D\n",
    "        Qreg_s_D[:,0] = np.array(vars[0:N])\n",
    "        Qreg = {'releases' : {'type'  : 'scheduling',\n",
    "                              'input' : Qreg_s_D},\n",
    "                'inflows'  : [],\n",
    "                'rel_inf'  : []}\n",
    "    \n",
    "        # Reservoir system simulation\n",
    "        Qenv, Qspill, Qreg_s_D, I_reg, s, E = res_sys_sim(I_fore, e_fore, s_ini, s_min, s_max,\n",
    "                                                          env_min, d_fore, Qreg)\n",
    "    \n",
    "        # Objective functions\n",
    "        SuDe = (np.maximum(d_fore-Qreg_s_D,0))**2   # supply deficit objective\n",
    "        StDi = s_max - s                            # storage difference objective\n",
    "        SSD = np.mean(SuDe[1:])                     # mean Squared Supply Deficit\n",
    "        SVD = np.mean(StDi[target_week])            # Storage Volume Difference\n",
    "    \n",
    "        return [SSD, SVD]\n",
    "    \n",
    "    problem = Problem(N,2)                     # Problem(number of optimizer variables, num of objective functions)\n",
    "    Real0 = Real(Qreg_s_D_min, Qreg_s_D_max)     # Range of values\n",
    "    \n",
    "    problem.types[:] = [Real0]*(N)\n",
    "    problem.function = auto_optim\n",
    "    \n",
    "    algorithm_1 = NSGAII(problem,pop_size)\n",
    "    algorithm_1.run(num_iter)\n",
    "    \n",
    "    sol_optim_1 = [algorithm_1.result[i].variables for i in range(pop_size)]\n",
    "    \n",
    "    rel_schedule = pd.DataFrame(sol_optim_1)               # array? csv? ???? ?? ???? ?? ??? ????? ?????\n",
    "    rel_schedule = rel_schedule.transpose()\n",
    "    rel_schedule['mean'] = rel_schedule.mean(numeric_only=True, axis=1)\n",
    "    rel_schedule.to_csv(output_path + '[opt_rel_schedule]' + reservoir_name + '_' + str(year) + '_' + str(month) \n",
    "                       + '_decision_' + str(decision_time) + 'm.csv')\n",
    "    \n",
    "    \n",
    "    results_SSD_1 = np.array([algorithm_1.result[i].objectives[0] for i in range(pop_size)])\n",
    "    results_SVD_1 = np.array([algorithm_1.result[i].objectives[1] for i in range(pop_size)])\n",
    "        \n",
    "    SSD = pd.DataFrame(results_SSD_1).rename(columns = {0:'SSD'})                      # array? csv? ???? ?? ???? ?? ??? ????? ?????\n",
    "    SVD = pd.DataFrame(results_SVD_1).rename(columns = {0:'SVD'})                      # array? csv? ???? ?? ???? ?? ??? ????? ?????\n",
    "    SSD_SVD = pd.concat([SSD, SVD], axis=1)\n",
    "    SSD_SVD.to_csv(output_path + '[sim_pareto]' + reservoir_name + '_' + str(year) + '_' + str(month) \n",
    "                   + '_decision_' + str(decision_time) + 'm.csv')\n",
    "            \n",
    "    temp = pd.DataFrame()\n",
    "    \n",
    "    for i in range(0,pop_size):\n",
    "        Qreg_s_D_opt = np.reshape(sol_optim_1[i], (N,1))\n",
    "        # Decision vector: regulated release to meet the demand in D\n",
    "    \n",
    "        Qreg_opt = {'releases' : {'type'  : 'scheduling',\n",
    "                                'input' : Qreg_s_D_opt},\n",
    "                'inflows'  : [],\n",
    "                'rel_inf'  : []}\n",
    "        \n",
    "        Qenv, Qspill, Qreg_s_D_opt, I_reg, s, E = res_sys_sim(I_obs, e_obs, s_ini, s_min, s_max, \n",
    "                                                              env_min, d_fore, Qreg_opt)\n",
    "        \n",
    "        SuDe = (np.maximum(d_fore-Qreg_s_D_opt,0))**2   # supply deficit objective\n",
    "        StDi = s_max - s                                # resource deficit objective\n",
    "        SSD = np.mean(SuDe[1:])\n",
    "        SVD = np.mean(StDi[target_week])\n",
    "        \n",
    "        output = pd.DataFrame(columns=['i', 'SSD', 'SVD'])    # array? csv? ???? ?? ???? ?? ??? ????? ?????\n",
    "        output.loc[i] = [i, round(SSD, 3), round(SVD, 3)]\n",
    "    \n",
    "        temp = pd.concat([temp, output], axis=0)\n",
    "    temp.set_index('i', inplace=True)\n",
    "    temp.to_csv(output_path + '[act_pareto]' + reservoir_name + '_' + str(year) + '_' + str(month) \n",
    "                   + '_decision_' + str(decision_time) + 'm.csv')\n",
    "\n",
    "    # Visualsing Actual Pareto-front -------------------------------------------------------------------------------------------------------------------------------------\n",
    "    plt.figure(figsize = (6,5))\n",
    "    plt.grid(True, axis='y', linestyle=':')\n",
    "    plt.grid(False, axis='x', linestyle=':')\n",
    "    \n",
    "    plt.scatter('SSD', 'SVD', data=temp, color='blue', alpha=0.21, s=50)\n",
    "    plt.title(reservoir_name + ' ('+ str(month) + '/' + str(year) + ')', x=0.5, y=0.91)\n",
    "    plt.xlabel('(SSD) Squared Supply Defict [MCM^2]')\n",
    "    plt.ylabel('(SVD) Storage Volume Difference [MCM]')\n",
    "    plt.savefig(output_path + '[act_pareto]' + reservoir_name + '_' + str(year) + '_' + str(month) \n",
    "                        + '_decision_' + str(decision_time) + 'm.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close('all')\n",
    "    \n",
    "    # Operation data management -------------------------------------------------------------------------------------------------------------------------------------\n",
    "    df = pd.read_csv(output_path + '[opt_rel_schedule]' + reservoir_name + '_' + str(year) + '_' + str(month) \n",
    "                       + '_decision_' + str(decision_time) + 'm.csv', index_col = [0])\n",
    "\n",
    "    Iobs = df.copy()\n",
    "    for i in range(0,pop_size):\n",
    "        Iobs[str(i)] = I_obs\n",
    "        Iobs['mean'] = I_obs\n",
    "    \n",
    "    Demand = df.copy()\n",
    "    for i in range(0,pop_size):\n",
    "        Demand[str(i)] = d_fore        \n",
    "        Demand['mean'] = d_fore\n",
    "        \n",
    "    Sini = df.copy().drop(df.index[1:])\n",
    "    for i in range(0,pop_size):\n",
    "        Sini[str(i)] = s_ini\n",
    "        Sini['mean'] = s_ini\n",
    "        \n",
    "    df_merge = pd.concat([df, Iobs, Demand, Sini], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    for i in range(0,len(df)): #storage volume calculation\n",
    "        df_merge.loc[len(df)*3+1+i] = np.minimum(df_merge.loc[len(df)*3+i] - df_merge.loc[i] + df_merge.loc[len(df)+i], s_max)\n",
    "        \n",
    "    for i in range(0,len(df)+1): #storage volume difference calculation\n",
    "        df_merge.loc[len(df)*4+1+i] = s_max - df_merge.loc[len(df)*3+i]\n",
    "        \n",
    "    for i in range(0,len(df)): #supply deficit calculation\n",
    "        df_merge.loc[len(df)*5+2+i] =  np.maximum(df_merge.loc[len(df)*2+i] - df_merge.loc[i],0)\n",
    "        \n",
    "    df_merge.loc[len(df)*6+2] = df_merge.iloc[len(df)*5+2:len(df)*6+2].sum() # supply deficit sum calculation\n",
    "    df_merge.loc[len(df)*6+3] = df_merge.iloc[len(df)*4+2:len(df)*5+2].mean() # mean SVD\n",
    "    df_merge.loc[len(df)*6+4] = df_merge.iloc[len(df)*5+3:len(df)*6+2].pow(2).mean() # mean SSD\n",
    "    df_merge.loc[len(df)*6+5] = df_merge.iloc[len(df)*5+3:len(df)*6+2].pow(2).sum() # mean SSD\n",
    "    \n",
    "    df_merge['remark'] = np.NaN\n",
    "    for i in range(0, len(df)):\n",
    "        df_merge['remark'].loc[i] = 'Release'\n",
    "        df_merge['remark'].loc[len(df) + i] = 'I_obs'\n",
    "        df_merge['remark'].loc[len(df)*2 + i] = 'Demand'\n",
    "        df_merge['remark'].loc[len(df)*3] = 'Initial Sotrage'\n",
    "        df_merge['remark'].loc[len(df)*3 + 1 + i] = 'Storage'\n",
    "    for i in range(0, len(df)+1):\n",
    "        df_merge['remark'].loc[len(df)*4 + 1 + i] = 'Volume Difference'\n",
    "        df_merge['remark'].loc[len(df)*5 + 1 + i+1] = 'Supply Deficit'\n",
    "        df_merge['remark'].loc[len(df)*6 + 1+1] = 'Sum of Supply Deficit'\n",
    "        df_merge['remark'].loc[len(df)*6 + 2+1] = 'mean SVD'\n",
    "        df_merge['remark'].loc[len(df)*6 + 3+1] = 'mean SSD'\n",
    "        df_merge['remark'].loc[len(df)*6 + 4+1] = 'sum SSD'\n",
    "    df_merge = df_merge.set_index('remark')\n",
    "    \n",
    "    df_merge.to_csv(output_path + '[Res_operation]' + reservoir_name + '_' + str(year) + '_' + str(month) \n",
    "                   + '_decision_' + str(decision_time) + 'm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reservoir_opt_probabilistic(reservoir_name,month,year,leadtime,pop, itnum, scenario):\n",
    "    \n",
    "    input_path = path + '/data/' + scenario_list[scenario] + '/in/leadtime_' + str(leadtime) + '/' # folder containing the forecast weather data\n",
    "    observation_path = path + '/data/' + scenario_list[1] + '/in/leadtime_' + str(leadtime) + '/' # folder containing the forecast weather data    \n",
    "    output_path = path + '/data/' + scenario_list[scenario] + '/out/leadtime_' + str(leadtime) + '/' + MCDM_list[MCDM] + '/' # folder containing the forecast weather data\n",
    "    \n",
    "    # Read potential evapotranspiration forecasts\n",
    "    e_fore_file = 'e_fore_' + str(year) + '_' + str(month) + '.csv'\n",
    "    dates_fore_day, e_fore_day = read_csv_data(input_path, e_fore_file)\n",
    "    dates_fore, e_fore_ens, e_fore_cum_ens = day2week(dates_fore_day, e_fore_day)\n",
    "    e_fore = np.zeros((dates_fore.shape[0],1))\n",
    "    e_fore[:,0] = e_fore_ens.mean(1)   # Average of the forecast ensemble\n",
    "    \n",
    "    # Read potential evapotranspiration observation\n",
    "    e_obs_file = 'e_fore_' + str(year) + '_' + str(month) + '.csv'\n",
    "    dates_obs_day, e_obs_day = read_csv_data(observation_path, e_obs_file)\n",
    "    dates_obs, e_obs_ens, e_obs_cum_ens = day2week(dates_obs_day,e_obs_day)\n",
    "    e_obs = np.zeros((dates_obs.shape[0],1))\n",
    "    e_obs[:,0] = e_obs_ens.mean(1)      # Average of the forecast ensemble\n",
    "\n",
    "    # Read inflow forecasts\n",
    "    I_fore_file = 'I_fore_'  + str(year) + '_' + str(month) + '.csv'\n",
    "    dates_fore_day, I_fore_day = read_csv_data(input_path, I_fore_file)\n",
    "    dates_fore, I_fore_ens, I_fore_cum_ens = day2week(dates_fore_day,I_fore_day)\n",
    "    I_fore = np.zeros((dates_fore.shape[0],1))\n",
    "    I_fore[:,0] = I_fore_ens.mean(1)      # Average of the forecast ensemble\n",
    "\n",
    "    # Read inflow observation\n",
    "    I_obs_file = 'I_fore_'  + str(year) + '_' + str(month) + '.csv'\n",
    "    dates_obs_day, I_obs_day = read_csv_data(observation_path, I_obs_file)\n",
    "    dates_obs, I_obs_ens, I_obs_cum_ens = day2week(dates_obs_day,I_obs_day)\n",
    "    I_obs = np.zeros((dates_obs.shape[0],1))\n",
    "    I_obs[:,0] = I_obs_ens.mean(1)         # Average of the forecast ensemble\n",
    "\n",
    "    # Read demand (fixed)\n",
    "    d_fore_file = 'd_fore_'  + str(year) + '_' + str(month) + '.csv'\n",
    "    dates_fore_day, d_fore_day = read_csv_data(input_path, d_fore_file)\n",
    "    dates_fore, d_fore_ens, d_fore_cum_ens = day2week(dates_fore_day,d_fore_day)\n",
    "    d_fore = np.zeros((dates_fore.shape[0],1))\n",
    "    d_fore[:,0] = d_fore_ens.mean(1)        # Average of the forecast ensemble\n",
    "\n",
    "    # Read observation data and set initial storage\n",
    "    obsdata = pd.read_csv(path + '/data/observed_data.csv',index_col = 'Date')\n",
    "    obsdata.index = pd.to_datetime(obsdata.index)\n",
    "        \n",
    "    if year == start_year and month == start_month :\n",
    "        ini_date = dates_fore[0] # the initial date corresponds to the first date of the weekly forecast\n",
    "        s_ini = obsdata[obsdata.index == ini_date].iloc[0,0]    \n",
    "    else:\n",
    "        if scenario == 1:\n",
    "            s_ini = S_all['S_pf'][len(S_all)-1]\n",
    "        elif scenario == 2:\n",
    "            s_ini = S_all['S_wc'][len(S_all)-1]\n",
    "        elif scenario == 3:\n",
    "            s_ini = S_all['S_d20'][len(S_all) - 1]\n",
    "        elif scenario == 4:\n",
    "            s_ini = S_all['S_esp'][len(S_all)-1]\n",
    "        elif scenario == 5:\n",
    "            s_ini = S_all['S_sffs'][len(S_all)-1]   \n",
    "    \n",
    "    M = I_fore_ens.shape[1] # M = Number of Ensemble or scenario\n",
    "    N = dates_fore.shape[0]                 # Number of forecast weeks\n",
    "    \n",
    "    # Define the target week for Storage Optimisation\n",
    "    if month + leadtime <10:\n",
    "        target_week = N\n",
    "    elif month >=10:\n",
    "        target_week = N\n",
    "    else:\n",
    "        target_week = N - 4*(month + leadtime - 10)\n",
    "    \n",
    "    \n",
    "    # Read observation data\n",
    "    obsdata = pd.read_csv(path + '/data/observed_data.csv',index_col = 'Date')\n",
    "    obsdata.index = pd.to_datetime(obsdata.index)\n",
    "    \n",
    "    # Release optimisation\n",
    "    pop_size = pop   # population size\n",
    "    num_iter = itnum # Number of iterations\n",
    "    \n",
    "    SSD = 0\n",
    "    SVD = 0\n",
    "                \n",
    "    def auto_optim(vars):\n",
    "        Qreg_S_D = np.array(vars[0:N]).reshape(N,1)\n",
    "        # Decision vector: regulated release to meet the demand in D\n",
    "        Qreg = {'releases' : {'type'  : 'scheduling',\n",
    "                                'input' : Qreg_S_D},\n",
    "                'inflows'  : [],\n",
    "                'rel_inf'  : []}\n",
    "                \n",
    "        # Reservoir system simulation\n",
    "        Qenv, Qspill, Qreg_S_D, I_reg, s, E = res_sys_sim(I_fore_ens,e_fore_ens, s_ini,s_min,s_max, \n",
    "                                                          env_min,d_fore, Qreg)\n",
    "                \n",
    "        # Objective functions\n",
    "        SuDe = (np.maximum(d_fore_ens-Qreg_S_D,0))**2   # supply deficit objective\n",
    "        StDi = s_max - s                            # resource deficit objective\n",
    "        SSD = np.mean(np.mean(SuDe[1:,:],axis = 0))\n",
    "        SVD = np.mean(np.mean(StDi[target_week,:],axis = 0))\n",
    "                \n",
    "        return [SSD, SVD]\n",
    "\n",
    "    problem = Problem(N,2)                     # Problem(number of optimizer variables, num of objective functions)\n",
    "    Real0 = Real(Qreg_s_D_min, Qreg_s_D_max)     # Range of values\n",
    "    \n",
    "    problem.types[:] = [Real0]*(N)\n",
    "    problem.function = auto_optim\n",
    "    \n",
    "    algorithm_2 = NSGAII(problem,pop_size)\n",
    "    algorithm_2.run(num_iter)\n",
    "    \n",
    "    sol_optim_2 = np.array([algorithm_2.result[i].variables for i in range(pop_size)])\n",
    "    \n",
    "    rel_schedule = pd.DataFrame(sol_optim_2)               # array? csv? ???? ?? ???? ?? ??? ????? ?????\n",
    "    rel_schedule = rel_schedule.transpose()\n",
    "    rel_schedule['mean'] = rel_schedule.mean(numeric_only=True, axis=1)\n",
    "    rel_schedule.to_csv(output_path + '[opt_rel_schedule]' + reservoir_name + '_' + str(year) + '_' + str(month) \n",
    "                       + '_decision_' + str(decision_time) + 'm.csv')\n",
    "    \n",
    "    \n",
    "    results_SSD_2 = np.array([algorithm_2.result[i].objectives[0] for i in range(pop_size)])\n",
    "    results_SVD_2 = np.array([algorithm_2.result[i].objectives[1] for i in range(pop_size)])\n",
    "        \n",
    "    SSD = pd.DataFrame(results_SSD_2).rename(columns = {0:'SSD'})                      # array? csv? ???? ?? ???? ?? ??? ????? ?????\n",
    "    SVD = pd.DataFrame(results_SVD_2).rename(columns = {0:'SVD'})                      # array? csv? ???? ?? ???? ?? ??? ????? ?????\n",
    "    SSD_SVD = pd.concat([SSD, SVD], axis=1)\n",
    "    SSD_SVD.to_csv(output_path + '[sim_pareto]' + reservoir_name + '_' + str(year) + '_' + str(month) \n",
    "                   + '_decision_' + str(decision_time) + 'm.csv')\n",
    "            \n",
    "    temp = pd.DataFrame()\n",
    "    \n",
    "    for i in range(0,pop_size):\n",
    "        Qreg_s_D_opt = np.reshape(sol_optim_2[i], (N,1))\n",
    "        # Decision vector: regulated release to meet the demand in D\n",
    "    \n",
    "        Qreg_opt = {'releases' : {'type'  : 'scheduling',\n",
    "                                'input' : Qreg_s_D_opt},\n",
    "                'inflows'  : [],\n",
    "                'rel_inf'  : []}\n",
    "        Qenv, Qspill, Qreg_s_D_opt, I_reg, s, E = res_sys_sim(I_obs, e_obs, s_ini, s_min, s_max, \n",
    "                                                              env_min, d_fore, Qreg_opt)\n",
    "        \n",
    "        SuDe = (np.maximum(d_fore-Qreg_s_D_opt,0))**2   # supply deficit objective\n",
    "        StDi = s_max - s                                # resource deficit objective\n",
    "        SSD = np.mean(SuDe[1:])\n",
    "        SVD = np.mean(StDi[target_week])\n",
    "        \n",
    "        output = pd.DataFrame(columns=['i', 'SSD', 'SVD'])    # array? csv? ???? ?? ???? ?? ??? ????? ?????\n",
    "        output.loc[i] = [i, round(SSD, 3), round(SVD, 3)]\n",
    "    \n",
    "        temp = pd.concat([temp, output], axis=0)\n",
    "    temp.set_index('i', inplace=True)\n",
    "    temp.to_csv(output_path + '[act_pareto]' + reservoir_name + '_' + str(year) + '_' + str(month) \n",
    "                   + '_decision_' + str(decision_time) + 'm.csv')\n",
    "\n",
    "    # Visualsing Actual Pareto-front -------------------------------------------------------------------------------------------------------------------------------------\n",
    "    plt.figure(figsize = (6,5))\n",
    "    plt.grid(True, axis='y', linestyle=':')\n",
    "    plt.grid(False, axis='x', linestyle=':')\n",
    "    \n",
    "    plt.scatter('SSD', 'SVD', data=temp, color='blue', alpha=0.21, s=50)\n",
    "    plt.title(reservoir_name + ' ('+ str(month) + '/' + str(year) + ')', x=0.5, y=0.91)\n",
    "    plt.xlabel('(SSD) Squared Supply Defict [MCM^2]')\n",
    "    plt.ylabel('(SVD) Storage Volume Difference [MCM]')\n",
    "    plt.savefig(output_path + '[act_pareto]' + reservoir_name + '_' + str(year) + '_' + str(month) \n",
    "                        + '_decision_' + str(decision_time) + 'm.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close('all')\n",
    "    \n",
    "    # Operation data management -------------------------------------------------------------------------------------------------------------------------------------\n",
    "    df = pd.read_csv(output_path + '[opt_rel_schedule]' + reservoir_name + '_' + str(year) + '_' + str(month) \n",
    "                       + '_decision_' + str(decision_time) + 'm.csv', index_col = [0])\n",
    "\n",
    "    Iobs = df.copy()\n",
    "    for i in range(0,pop_size):\n",
    "        Iobs[str(i)] = I_obs_ens\n",
    "        Iobs['mean'] = I_obs_ens\n",
    "    \n",
    "    Demand = df.copy()\n",
    "    for i in range(0,pop_size):\n",
    "        Demand[str(i)] = d_fore        \n",
    "        Demand['mean'] = d_fore\n",
    "        \n",
    "    Sini = df.copy().drop(df.index[1:])\n",
    "    for i in range(0,pop_size):\n",
    "        Sini[str(i)] = s_ini\n",
    "        Sini['mean'] = s_ini\n",
    "        \n",
    "    df_merge = pd.concat([df, Iobs, Demand, Sini], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    for i in range(0,len(df)): #storage volume calculation\n",
    "        df_merge.loc[len(df)*3+1+i] = np.minimum(df_merge.loc[len(df)*3+i] - df_merge.loc[i] + df_merge.loc[len(df)+i], s_max)\n",
    "        \n",
    "    for i in range(0,len(df)+1): #storage volume difference calculation\n",
    "        df_merge.loc[len(df)*4+1+i] = s_max - df_merge.loc[len(df)*3+i]\n",
    "        \n",
    "    for i in range(0,len(df)): #supply deficit calculation\n",
    "        df_merge.loc[len(df)*5+2+i] =  np.maximum(df_merge.loc[len(df)*2+i] - df_merge.loc[i],0)\n",
    "        \n",
    "    df_merge.loc[len(df)*6+2] = df_merge.iloc[len(df)*5+2:len(df)*6+2].sum() # supply deficit sum calculation\n",
    "    df_merge.loc[len(df)*6+3] = df_merge.iloc[len(df)*4+2:len(df)*5+2].mean() # mean SVD\n",
    "    df_merge.loc[len(df)*6+4] = df_merge.iloc[len(df)*5+3:len(df)*6+2].pow(2).mean() # mean SSD\n",
    "    df_merge.loc[len(df)*6+5] = df_merge.iloc[len(df)*5+3:len(df)*6+2].pow(2).sum() # mean SSD\n",
    "    \n",
    "    df_merge['remark'] = np.NaN\n",
    "    for i in range(0, len(df)):\n",
    "        df_merge['remark'].loc[i] = 'Release'\n",
    "        df_merge['remark'].loc[len(df) + i] = 'I_obs'\n",
    "        df_merge['remark'].loc[len(df)*2 + i] = 'Demand'\n",
    "        df_merge['remark'].loc[len(df)*3] = 'Initial Sotrage'\n",
    "        df_merge['remark'].loc[len(df)*3 + 1 + i] = 'Storage'\n",
    "    for i in range(0, len(df)+1):\n",
    "        df_merge['remark'].loc[len(df)*4 + 1 + i] = 'Volume Difference'\n",
    "        df_merge['remark'].loc[len(df)*5 + 1 + i+1] = 'Supply Deficit'\n",
    "        df_merge['remark'].loc[len(df)*6 + 1+1] = 'Sum of Supply Deficit'\n",
    "        df_merge['remark'].loc[len(df)*6 + 2+1] = 'mean SVD'\n",
    "        df_merge['remark'].loc[len(df)*6 + 3+1] = 'mean SSD'\n",
    "        df_merge['remark'].loc[len(df)*6 + 4+1] = 'sum SSD'\n",
    "    df_merge = df_merge.set_index('remark')\n",
    "    \n",
    "    df_merge.to_csv(output_path + '[Res_operation]' + reservoir_name + '_' + str(year) + '_' + str(month) \n",
    "                   + '_decision_' + str(decision_time) + 'm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Functions for MCDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCDM_SAW(MCDM, year, month, reservoir_name, decision_time, leadtime):\n",
    "    \n",
    "    # Define file paths for different flow scenarios/forecasts\n",
    "    path_pf = path + '/data/' + scenario_list[1] + '/out/leadtime_' + str(leadtime) + '/' + MCDM_list[MCDM]   # Perfect forecast\n",
    "    path_wc = path + '/data/' + scenario_list[2] + '/out/leadtime_' + str(leadtime) + '/' + MCDM_list[MCDM]   # Worst-case\n",
    "    path_d20 = path + '/data/' + scenario_list[3] + '/out/leadtime_' + str(leadtime) + '/' + MCDM_list[MCDM] # 20-year return period drought\n",
    "    path_esp = path + '/data/' + scenario_list[4] + '/out/leadtime_' + str(leadtime) + '/' + MCDM_list[MCDM]  # ESP (Ensemble Streamflow Prediction)\n",
    "    path_sffs = path + '/data/' + scenario_list[5] + '/out/leadtime_' + str(leadtime) + '/' + MCDM_list[MCDM] # SFFs (Synthetic Flow Forecasts)\n",
    "\n",
    "    # Read simulated Pareto front data for each flow scenario/forecast\n",
    "    sim_pf = pd.read_csv(path_pf + '/[sim_pareto]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    sim_wc = pd.read_csv(path_wc + '/[sim_pareto]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    sim_d20 = pd.read_csv(path_d20 + '/[sim_pareto]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    sim_esp = pd.read_csv(path_esp + '/[sim_pareto]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    sim_sffs = pd.read_csv(path_sffs + '/[sim_pareto]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "\n",
    "    # Normalization of the simulated Pareto front data\n",
    "    scaler = MinMaxScaler()\n",
    "    sim_all = pd.concat([sim_pf, sim_wc, sim_d20, sim_esp, sim_sffs], axis=0)  # Combine data from all scenarios\n",
    "    nor_all = pd.DataFrame(scaler.fit(sim_all).transform(sim_all)).rename({0: 'SSD', 1: 'SVD'}, axis=1)  # Normalize data\n",
    "    nor_all['number'] = nor_all.index  # Add index column for identification\n",
    "    nor_all['remark'] = 'Perfect forecast'  # Default remark for Perfect forecast scenario\n",
    "    \n",
    "    # Adjust remarks and indices for different scenarios\n",
    "    for i in range(len(nor_all)):\n",
    "        if (nor_all['number'][i] >= 100) and (nor_all['number'][i] < 200):\n",
    "            nor_all['remark'][i] = 'Worst-case'\n",
    "            nor_all['number'][i] = nor_all['number'][i] - 100\n",
    "        elif (nor_all['number'][i] >= 200) and (nor_all['number'][i] < 300):\n",
    "            nor_all['remark'][i] = '20-year drought'\n",
    "            nor_all['number'][i] = nor_all['number'][i] - 200\n",
    "        elif (nor_all['number'][i] >= 300) and (nor_all['number'][i] < 400):\n",
    "            nor_all['remark'][i] = 'ESP'\n",
    "            nor_all['number'][i] = nor_all['number'][i] - 300\n",
    "        elif (nor_all['number'][i] >= 400) and (nor_all['number'][i] < 500):\n",
    "            nor_all['remark'][i] = 'SFFs'\n",
    "            nor_all['number'][i] = nor_all['number'][i] - 400\n",
    "    \n",
    "    # Assign weights based on the selected MCDM method\n",
    "    if MCDM == 1:  # Balanced\n",
    "        weight = weight_bal\n",
    "    elif MCDM == 2:  # Supply prioritized\n",
    "        weight = weight_sup\n",
    "    elif MCDM == 3:  # Storage prioritized\n",
    "        weight = weight_sto\n",
    "\n",
    "    # Calculate scores and find the optimal number for each scenario based on SAW methods\n",
    "    nor_pf = nor_all[(nor_all.remark == 'Perfect forecast')]\n",
    "    nor_pf['score'] = abs(nor_pf['SSD'] * (1 - weight) + nor_pf['SVD'] * weight)\n",
    "    opt_num_pf = nor_pf[(nor_pf['score'] == nor_pf['score'].min())].index[0]\n",
    "\n",
    "    nor_wc = nor_all[(nor_all.remark == 'Worst-case')]\n",
    "    nor_wc['score'] = abs(nor_wc['SSD'] * (1 - weight) + nor_wc['SVD'] * weight)\n",
    "    opt_num_wc = nor_wc[(nor_wc['score'] == nor_wc['score'].min())].index[0]\n",
    "    \n",
    "    nor_d20 = nor_all[(nor_all.remark == '20-year drought')]\n",
    "    nor_d20['score'] = abs(nor_d20['SSD'] * weight + nor_d20['SVD'] * (1 - weight))\n",
    "    opt_num_d20 = nor_d20[(nor_d20['score'] == nor_d20['score'].min())].index[0]  # Find optimal solution        \n",
    "\n",
    "    nor_esp = nor_all[(nor_all.remark == 'ESP')]\n",
    "    nor_esp['score'] = abs(nor_esp['SSD'] * (1 - weight) + nor_esp['SVD'] * weight)\n",
    "    opt_num_esp = nor_esp[(nor_esp['score'] == nor_esp['score'].min())].index[0]\n",
    "\n",
    "    nor_sffs = nor_all[(nor_all.remark == 'SFFs')]\n",
    "    nor_sffs['score'] = abs(nor_sffs['SSD'] * (1 - weight) + nor_sffs['SVD'] * weight)\n",
    "    opt_num_sffs = nor_sffs[(nor_sffs['score'] == nor_sffs['score'].min())].index[0]\n",
    "\n",
    "    # Load and save reservoir operation data for each scenario using the optimal numbers\n",
    "    pf = pd.read_csv(path_pf + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    pf[str(opt_num_pf)].to_csv(path_pf + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm_opt.csv')\n",
    "    S_pf = pd.DataFrame(pf[str(opt_num_pf)].loc['Storage'])[:].rename(columns={str(opt_num_pf): 'S_pf'})[:decision_time * 4 + 1]\n",
    "\n",
    "    wc = pd.read_csv(path_wc + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    wc[str(opt_num_wc)].to_csv(path_wc + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm_opt.csv')\n",
    "    S_wc = pd.DataFrame(wc[str(opt_num_wc)].loc['Storage'])[:].rename(columns={str(opt_num_wc): 'S_wc'})[:decision_time * 4 + 1]\n",
    "\n",
    "    d20 = pd.read_csv(path_d20 + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    d20[str(opt_num_d20)].to_csv(path_d20 + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm_opt.csv')\n",
    "    S_d20 = pd.DataFrame(d20[str(opt_num_d20)].loc['Storage'])[:].rename(columns={str(opt_num_d20): 'S_d20'})[:decision_time * 4 + 1]    \n",
    "        \n",
    "    esp = pd.read_csv(path_esp + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    esp[str(opt_num_esp)].to_csv(path_esp + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm_opt.csv')\n",
    "    S_esp = pd.DataFrame(esp[str(opt_num_esp)].loc['Storage'])[:].rename(columns={str(opt_num_esp): 'S_esp'})[:decision_time * 4 + 1]\n",
    "\n",
    "    sffs = pd.read_csv(path_sffs + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    sffs[str(opt_num_sffs)].to_csv(path_sffs + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm_opt.csv')\n",
    "    S_sffs = pd.DataFrame(sffs[str(opt_num_sffs)].loc['Storage'])[:].rename(columns={str(opt_num_sffs): 'S_sffs'})[:decision_time * 4 + 1]\n",
    "\n",
    "    # Combine storage data from all scenarios into a single DataFrame and return\n",
    "    S_all = pd.concat([S_pf, S_wc, S_d20, S_esp, S_sffs], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    return S_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub function for Simple Selective method\n",
    "def get_weight_ss(S_value):\n",
    "    weight_mapping = {                                    # assign methods depending on predefined storage range\n",
    "        (0, 2700): (1-weight_sto, weight_sto),            # Storage-prioritized \n",
    "        (2700, 3300): (1-weight_bal, weight_bal),           # Balanced\n",
    "        (3300, float('inf')): (1-weight_sup, weight_sup)  # Supply-prioritized\n",
    "    }\n",
    "    for range_tuple, weights in weight_mapping.items():\n",
    "        if range_tuple[0] < S_value <= range_tuple[1]:\n",
    "            return weights\n",
    "    return (0.5, 0.5)  # Í∏∞Î≥∏Í∞í (SÍ∞íÏù¥ Ï£ºÏñ¥ÏßÑ Î≤îÏúÑÏóê ÏóÜÎäî Í≤ΩÏö∞)\n",
    "\n",
    "# sub function for Multi-weighting method\n",
    "def get_weight_mw(S_value):\n",
    "    weight_mapping = {                      # assign weights depending on predefined storage range\n",
    "        (0, 2500): (0.3, 0.7),          \n",
    "        (2500, 2750): (0.4, 0.6),\n",
    "        (2750, 3000): (0.5, 0.5),\n",
    "        (3000, 3250): (0.6, 0.4),\n",
    "        (3250, 3500): (0.7, 0.3),\n",
    "        (3500, float('inf')): (0.8, 0.2)\n",
    "    }\n",
    "    for range_tuple, weights in weight_mapping.items():\n",
    "        if range_tuple[0] < S_value <= range_tuple[1]:\n",
    "            return weights\n",
    "    return (0.5, 0.5)  # Í∏∞Î≥∏Í∞í (SÍ∞íÏù¥ Ï£ºÏñ¥ÏßÑ Î≤îÏúÑÏóê ÏóÜÎäî Í≤ΩÏö∞)\n",
    "\n",
    "# Main function for Variable Weighting methods\n",
    "def MCDM_VW(MCDM, year, month, reservoir_name, decision_time, leadtime, scenario):\n",
    "    # Define file paths for different flow scenarios/forecasts\n",
    "    path_pf = path + '/data/' + scenario_list[1] + '/out/leadtime_' + str(leadtime) + '/' + MCDM_list[MCDM]   # Perfect forecast\n",
    "    path_wc = path + '/data/' + scenario_list[2] + '/out/leadtime_' + str(leadtime) + '/' + MCDM_list[MCDM]   # Worst-case\n",
    "    path_d20 = path + '/data/' + scenario_list[3] + '/out/leadtime_' + str(leadtime) + '/' + MCDM_list[MCDM] # 20-year return period drought\n",
    "    path_esp = path + '/data/' + scenario_list[4] + '/out/leadtime_' + str(leadtime) + '/' + MCDM_list[MCDM]  # ESP (Ensemble Streamflow Prediction)\n",
    "    path_sffs = path + '/data/' + scenario_list[5] + '/out/leadtime_' + str(leadtime) + '/' + MCDM_list[MCDM] # SFFs (Synthetic Flow Forecasts)\n",
    "\n",
    "    # Read simulated Pareto front data for each flow scenario/forecast\n",
    "    sim_pf = pd.read_csv(path_pf + '/[sim_pareto]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    sim_wc = pd.read_csv(path_wc + '/[sim_pareto]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    sim_d20 = pd.read_csv(path_d20 + '/[sim_pareto]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    sim_esp = pd.read_csv(path_esp + '/[sim_pareto]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    sim_sffs = pd.read_csv(path_sffs + '/[sim_pareto]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    \n",
    "    # Normalization of the simulated Pareto front data\n",
    "    scaler = MinMaxScaler()\n",
    "    sim_all = pd.concat([sim_pf, sim_wc, sim_d20, sim_esp, sim_sffs], axis=0)  # Combine data from all scenarios\n",
    "    nor_all = pd.DataFrame(scaler.fit(sim_all).transform(sim_all)).rename({0: 'SSD', 1: 'SVD'}, axis=1)  # Normalize data\n",
    "    nor_all['number'] = nor_all.index  # Add index column for identification\n",
    "    nor_all['remark'] = 'Perfect forecast'  # Default remark for Perfect forecast scenario\n",
    "    \n",
    "    for i in range(len(nor_all)):\n",
    "        if (nor_all['number'][i] >= 100) and (nor_all['number'][i] < 200):\n",
    "            nor_all['remark'][i] = 'Worst-case'\n",
    "            nor_all['number'][i] = nor_all['number'][i] - 100\n",
    "        elif (nor_all['number'][i] >= 200) and (nor_all['number'][i] < 300):\n",
    "            nor_all['remark'][i] = '20-year drought'\n",
    "            nor_all['number'][i] = nor_all['number'][i] - 200\n",
    "        elif (nor_all['number'][i] >= 300) and (nor_all['number'][i] < 400):\n",
    "            nor_all['remark'][i] = 'ESP'\n",
    "            nor_all['number'][i] = nor_all['number'][i] - 300\n",
    "        elif (nor_all['number'][i] >= 400) and (nor_all['number'][i] < 500):\n",
    "            nor_all['remark'][i] = 'SFFs'\n",
    "            nor_all['number'][i] = nor_all['number'][i] - 400\n",
    "    \n",
    "    if MCDM == 4:\n",
    "        w_ssd, w_svd = get_weight_ss(S_value)\n",
    "    elif MCDM == 5:\n",
    "        w_ssd, w_svd = get_weight_mw(S_value)\n",
    "    else:\n",
    "        drop\n",
    "\n",
    "    # Apply the scoring function to each scenario\n",
    "    nor_pf = nor_all[nor_all.remark == 'Perfect forecast']\n",
    "    nor_pf['score'] = abs(nor_pf['SSD'] * w_ssd + nor_pf['SVD'] * w_svd)\n",
    "    opt_num_pf = nor_pf[(nor_pf['score'] == nor_pf['score'].min())].index[0]\n",
    "\n",
    "    nor_wc = nor_all[nor_all.remark == 'Worst-case']\n",
    "    nor_wc['score'] = abs(nor_wc['SSD'] * w_ssd + nor_wc['SVD'] * w_svd)\n",
    "    opt_num_wc = nor_wc[(nor_wc['score'] == nor_wc['score'].min())].index[0]\n",
    "\n",
    "    nor_d20 = nor_all[nor_all.remark == '20-year drought']\n",
    "    nor_d20['score'] = abs(nor_d20['SSD'] * w_ssd + nor_d20['SVD'] * w_svd)\n",
    "    opt_num_d20 = nor_d20[(nor_d20['score'] == nor_d20['score'].min())].index[0]\n",
    "\n",
    "    nor_esp = nor_all[nor_all.remark == 'ESP']\n",
    "    nor_esp['score'] = abs(nor_esp['SSD'] * w_ssd + nor_esp['SVD'] * w_svd)\n",
    "    opt_num_esp = nor_esp[(nor_esp['score'] == nor_esp['score'].min())].index[0]\n",
    "\n",
    "    nor_sffs = nor_all[nor_all.remark == 'SFFs']\n",
    "    nor_sffs['score'] = abs(nor_sffs['SSD'] * w_ssd + nor_sffs['SVD'] * w_svd)\n",
    "    opt_num_sffs = nor_sffs[(nor_sffs['score'] == nor_sffs['score'].min())].index[0]\n",
    "    \n",
    "    # Load and save reservoir operation data for each scenario using the optimal numbers\n",
    "    pf = pd.read_csv(path_pf + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    pf[str(opt_num_pf)].to_csv(path_pf + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm_opt.csv')\n",
    "    S_pf = pd.DataFrame(pf[str(opt_num_pf)].loc['Storage'])[:].rename(columns={str(opt_num_pf): 'S_pf'})[:decision_time * 4 + 1]\n",
    "\n",
    "    wc = pd.read_csv(path_wc + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    wc[str(opt_num_wc)].to_csv(path_wc + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm_opt.csv')\n",
    "    S_wc = pd.DataFrame(wc[str(opt_num_wc)].loc['Storage'])[:].rename(columns={str(opt_num_wc): 'S_wc'})[:decision_time * 4 + 1]\n",
    "\n",
    "    d20 = pd.read_csv(path_d20 + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    d20[str(opt_num_d20)].to_csv(path_d20 + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm_opt.csv')\n",
    "    S_d20 = pd.DataFrame(d20[str(opt_num_d20)].loc['Storage'])[:].rename(columns={str(opt_num_d20): 'S_d20'})[:decision_time * 4 + 1]    \n",
    "        \n",
    "    esp = pd.read_csv(path_esp + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    esp[str(opt_num_esp)].to_csv(path_esp + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm_opt.csv')\n",
    "    S_esp = pd.DataFrame(esp[str(opt_num_esp)].loc['Storage'])[:].rename(columns={str(opt_num_esp): 'S_esp'})[:decision_time * 4 + 1]\n",
    "\n",
    "    sffs = pd.read_csv(path_sffs + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    sffs[str(opt_num_sffs)].to_csv(path_sffs + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm_opt.csv')\n",
    "    S_sffs = pd.DataFrame(sffs[str(opt_num_sffs)].loc['Storage'])[:].rename(columns={str(opt_num_sffs): 'S_sffs'})[:decision_time * 4 + 1]\n",
    "\n",
    "    # Combine storage data from all scenarios into a single DataFrame and return\n",
    "    S_all = pd.concat([S_pf, S_wc, S_d20, S_esp, S_sffs], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    return S_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function for Reference point methods\n",
    "def MCDM_RP(MCDM, year, month, reservoir_name, decision_time, leadtime):\n",
    "    # Define file paths for different flow scenarios/forecasts\n",
    "    path_pf = path + '/data/' + scenario_list[1] + '/out/leadtime_' + str(leadtime) + '/' + MCDM_list[MCDM]   # Perfect forecast\n",
    "    path_wc = path + '/data/' + scenario_list[2] + '/out/leadtime_' + str(leadtime) + '/' + MCDM_list[MCDM]   # Worst-case\n",
    "    path_d20 = path + '/data/' + scenario_list[3] + '/out/leadtime_' + str(leadtime) + '/' + MCDM_list[MCDM] # 20-year return period drought\n",
    "    path_esp = path + '/data/' + scenario_list[4] + '/out/leadtime_' + str(leadtime) + '/' + MCDM_list[MCDM]  # ESP (Ensemble Streamflow Prediction)\n",
    "    path_sffs = path + '/data/' + scenario_list[5] + '/out/leadtime_' + str(leadtime) + '/' + MCDM_list[MCDM] # SFFs (Synthetic Flow Forecasts)\n",
    "\n",
    "    # Read simulated Pareto front data for each flow scenario/forecast\n",
    "    sim_pf = pd.read_csv(path_pf + '/[sim_pareto]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    sim_wc = pd.read_csv(path_wc + '/[sim_pareto]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    sim_d20 = pd.read_csv(path_d20 + '/[sim_pareto]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    sim_esp = pd.read_csv(path_esp + '/[sim_pareto]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    sim_sffs = pd.read_csv(path_sffs + '/[sim_pareto]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "\n",
    "    # Normalization of the simulated Pareto front data\n",
    "    scaler = MinMaxScaler()\n",
    "    sim_all = pd.concat([sim_pf, sim_wc, sim_d20, sim_esp, sim_sffs], axis=0)  # Combine data from all scenarios\n",
    "    nor_all = pd.DataFrame(scaler.fit(sim_all).transform(sim_all)).rename({0: 'SSD', 1: 'SVD'}, axis=1)  # Normalize data\n",
    "    nor_all['number'] = nor_all.index  # Add index column for identification\n",
    "    nor_all['remark'] = 'Perfect forecast'  # Default remark for Perfect forecast scenario\n",
    "    \n",
    "    for i in range(len(nor_all)):\n",
    "        if (nor_all['number'][i] >= 100) and (nor_all['number'][i] < 200):\n",
    "            nor_all['remark'][i] = 'Worst-case'\n",
    "            nor_all['number'][i] = nor_all['number'][i] - 100\n",
    "        elif (nor_all['number'][i] >= 200) and (nor_all['number'][i] < 300):\n",
    "            nor_all['remark'][i] = '20-year drought'\n",
    "            nor_all['number'][i] = nor_all['number'][i] - 200\n",
    "        elif (nor_all['number'][i] >= 300) and (nor_all['number'][i] < 400):\n",
    "            nor_all['remark'][i] = 'ESP'\n",
    "            nor_all['number'][i] = nor_all['number'][i] - 300\n",
    "        elif (nor_all['number'][i] >= 400) and (nor_all['number'][i] < 500):\n",
    "            nor_all['remark'][i] = 'SFFs'\n",
    "            nor_all['number'][i] = nor_all['number'][i] - 400\n",
    "\n",
    "    nor_pf = nor_all[nor_all.remark == 'Perfect forecast']\n",
    "    if MCDM == 6:  # Utopian point method\n",
    "        nor_pf['distance'] = abs(nor_pf['SSD']**2 + nor_pf['SVD']**2)**0.5\n",
    "        opt_num_pf = nor_pf[(nor_pf['distance'] == nor_pf['distance'].min())].index[0]\n",
    "    elif MCDM == 7:  # Knee point method\n",
    "        nor_pf['distance'] = abs(nor_pf['SSD'] + nor_pf['SVD'])\n",
    "        opt_num_pf = nor_pf[(nor_pf['distance'] == nor_pf['distance'].min())].index[0]\n",
    "    elif MCDM == 8:  # TOPSIS method\n",
    "        nor_pf['distance+'] = abs(nor_pf['SSD']**2 + nor_pf['SVD']**2)**0.5\n",
    "        nor_pf['distance-'] = abs((1-nor_pf['SSD'])**2 + (1-nor_pf['SVD'])**2)**0.5\n",
    "        nor_pf['R_wc'] = nor_pf['distance-'] / (nor_pf['distance-']+nor_pf['distance+'])\n",
    "        opt_num_pf = nor_pf[(nor_pf['R_wc'] == nor_pf['R_wc'].max())].index[0]\n",
    "    else:\n",
    "        drop\n",
    "        \n",
    "    nor_wc = nor_all[nor_all.remark == 'Worst-case']    \n",
    "    if MCDM == 6:  # Utopian point method\n",
    "        nor_wc['distance'] = abs(nor_wc['SSD']**2 + nor_wc['SVD']**2)**0.5\n",
    "        opt_num_wc = nor_wc[(nor_wc['distance'] == nor_wc['distance'].min())].index[0]\n",
    "    elif MCDM == 7:  # Knee point method\n",
    "        nor_wc['distance'] = abs(nor_wc['SSD'] + nor_wc['SVD'])\n",
    "        opt_num_wc = nor_wc[(nor_wc['distance'] == nor_wc['distance'].min())].index[0]\n",
    "    elif MCDM == 8:  # TOPSIS method\n",
    "        nor_wc['distance+'] = abs(nor_wc['SSD']**2 + nor_wc['SVD']**2)**0.5\n",
    "        nor_wc['distance-'] = abs((1-nor_wc['SSD'])**2 + (1-nor_wc['SVD'])**2)**0.5\n",
    "        nor_wc['R_wc'] = nor_wc['distance-'] / (nor_wc['distance-']+nor_wc['distance+'])\n",
    "        opt_num_wc = nor_wc[(nor_wc['R_wc'] == nor_wc['R_wc'].max())].index[0]\n",
    "    else:\n",
    "        drop        \n",
    "\n",
    "    nor_d20 = nor_all[nor_all.remark == '20-year drought']    \n",
    "    if MCDM == 6:  # Utopian point method\n",
    "        nor_d20['distance'] = abs(nor_d20['SSD']**2 + nor_d20['SVD']**2)**0.5\n",
    "        opt_num_d20 = nor_d20[(nor_d20['distance'] == nor_d20['distance'].min())].index[0]\n",
    "    elif MCDM == 7:  # Knee point method\n",
    "        nor_d20['distance'] = abs(nor_d20['SSD'] + nor_d20['SVD'])\n",
    "        opt_num_d20 = nor_d20[(nor_d20['distance'] == nor_d20['distance'].min())].index[0]\n",
    "    elif MCDM == 8:  # TOPSIS method\n",
    "        nor_d20['distance+'] = abs(nor_d20['SSD']**2 + nor_d20['SVD']**2)**0.5\n",
    "        nor_d20['distance-'] = abs((1-nor_d20['SSD'])**2 + (1-nor_d20['SVD'])**2)**0.5\n",
    "        nor_d20['R_d20'] = nor_d20['distance-'] / (nor_d20['distance-']+nor_d20['distance+'])\n",
    "        opt_num_d20 = nor_d20[(nor_d20['R_d20'] == nor_d20['R_d20'].max())].index[0]\n",
    "    else:\n",
    "        drop        \n",
    "        \n",
    "    nor_esp = nor_all[nor_all.remark == 'ESP']\n",
    "    if MCDM == 6:  # Utopian point method\n",
    "        nor_esp['distance'] = abs(nor_esp['SSD']**2 + nor_esp['SVD']**2)**0.5\n",
    "        opt_num_esp = nor_esp[(nor_esp['distance'] == nor_esp['distance'].min())].index[0]\n",
    "    elif MCDM == 7:  # Knee point method\n",
    "        nor_esp['distance'] = abs(nor_esp['SSD'] + nor_esp['SVD'])\n",
    "        opt_num_esp = nor_esp[(nor_esp['distance'] == nor_esp['distance'].min())].index[0]\n",
    "    elif MCDM == 8:  # TOPSIS method\n",
    "        nor_esp['distance+'] = abs(nor_esp['SSD']**2 + nor_esp['SVD']**2)**0.5\n",
    "        nor_esp['distance-'] = abs((1-nor_esp['SSD'])**2 + (1-nor_esp['SVD'])**2)**0.5\n",
    "        nor_esp['R_esp'] = nor_esp['distance-'] / (nor_esp['distance-']+nor_esp['distance+'])\n",
    "        opt_num_esp = nor_esp[(nor_esp['R_esp'] == nor_esp['R_esp'].max())].index[0]\n",
    "    else:\n",
    "        drop        \n",
    "\n",
    "    nor_sffs = nor_all[nor_all.remark == 'SFFs']\n",
    "    if MCDM == 6:  # Utopian point method\n",
    "        nor_sffs['distance'] = abs(nor_sffs['SSD']**2 + nor_sffs['SVD']**2)**0.5\n",
    "        opt_num_sffs = nor_sffs[(nor_sffs['distance'] == nor_sffs['distance'].min())].index[0]\n",
    "    elif MCDM == 7:  # Knee point method\n",
    "        nor_sffs['distance'] = abs(nor_sffs['SSD'] + nor_sffs['SVD'])\n",
    "        opt_num_sffs = nor_sffs[(nor_sffs['distance'] == nor_sffs['distance'].min())].index[0]\n",
    "    elif MCDM == 8:  # TOPSIS method\n",
    "        nor_sffs['distance+'] = abs(nor_sffs['SSD']**2 + nor_sffs['SVD']**2)**0.5\n",
    "        nor_sffs['distance-'] = abs((1-nor_sffs['SSD'])**2 + (1-nor_sffs['SVD'])**2)**0.5\n",
    "        nor_sffs['R_sffs'] = nor_sffs['distance-'] / (nor_sffs['distance-']+nor_sffs['distance+'])\n",
    "        opt_num_sffs = nor_sffs[(nor_sffs['R_sffs'] == nor_sffs['R_sffs'].max())].index[0]\n",
    "    else:\n",
    "        drop        \n",
    "    \n",
    "    # Load and save reservoir operation data for each scenario using the optimal numbers\n",
    "    pf = pd.read_csv(path_pf + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    pf[str(opt_num_pf)].to_csv(path_pf + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm_opt.csv')\n",
    "    S_pf = pd.DataFrame(pf[str(opt_num_pf)].loc['Storage'])[:].rename(columns={str(opt_num_pf): 'S_pf'})[:decision_time * 4 + 1]\n",
    "\n",
    "    wc = pd.read_csv(path_wc + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    wc[str(opt_num_wc)].to_csv(path_wc + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm_opt.csv')\n",
    "    S_wc = pd.DataFrame(wc[str(opt_num_wc)].loc['Storage'])[:].rename(columns={str(opt_num_wc): 'S_wc'})[:decision_time * 4 + 1]\n",
    "\n",
    "    d20 = pd.read_csv(path_d20 + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    d20[str(opt_num_d20)].to_csv(path_d20 + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm_opt.csv')\n",
    "    S_d20 = pd.DataFrame(d20[str(opt_num_d20)].loc['Storage'])[:].rename(columns={str(opt_num_d20): 'S_d20'})[:decision_time * 4 + 1]    \n",
    "        \n",
    "    esp = pd.read_csv(path_esp + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    esp[str(opt_num_esp)].to_csv(path_esp + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm_opt.csv')\n",
    "    S_esp = pd.DataFrame(esp[str(opt_num_esp)].loc['Storage'])[:].rename(columns={str(opt_num_esp): 'S_esp'})[:decision_time * 4 + 1]\n",
    "\n",
    "    sffs = pd.read_csv(path_sffs + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm.csv', index_col=0)\n",
    "    sffs[str(opt_num_sffs)].to_csv(path_sffs + '/[Res_operation]' + str(reservoir_name) + '_' + str(year) + '_' + str(month) + '_decision_' + str(decision_time) + 'm_opt.csv')\n",
    "    S_sffs = pd.DataFrame(sffs[str(opt_num_sffs)].loc['Storage'])[:].rename(columns={str(opt_num_sffs): 'S_sffs'})[:decision_time * 4 + 1]\n",
    "\n",
    "    # Combine storage data from all scenarios and forecasts into a single DataFrame and return\n",
    "    S_all = pd.concat([S_pf, S_wc, S_d20, S_esp, S_sffs], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    return S_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Continuous reservoir simulations\n",
    "\n",
    "For the given simulation period, this code facilitates continuous release optimization by generating Pareto fronts and selecting a compromise release schedule using predefined functions. Please note that this process may require a significant amount of time, depending on the duration of the simulation, decision-making time step and number of iteration. For example, simulating over a period of 2.5 years can take approximately 6-7 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Reservoir simulation settings\n",
    "\n",
    "Now, it is necessary to specify the details of the reservoir simulations, including parameters such as storage names, capacity constraints, and release constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the flow scenario or forecast used in the analysis\n",
    "scenario_list = {1:'1. Perfect forecast scenario', 2:'2. Worst case scenario', 3:'3. 20-year drought', 4:'4. ESP', 5:'5. SFFs'} \n",
    "# Define the MCDM method used in the analysis. Details of these MCDM methods will be explained in the next notebook.\n",
    "MCDM_list = {1:'saw_bal', 2:'saw_sup', 3:'saw_sto', 4:'vw_select', 5:'vw_multi', 6:'rp_utopia', 7:'rp_knee', 8:'rp_topsis'}\n",
    "\n",
    "# Define reservoir characteristic\n",
    "reservoir_name = 'A'\n",
    "s_min = 337               # Minimum Storage volume (Million Cubic Meters)\n",
    "s_max = 4349              # Maximum Storage volume (Million Cubic Meters)\n",
    "Q_max = 652               # Maximum regulated release (Million Cubic Meters/day)\n",
    "env_min = 0               # Minimum environmental release (MCM/d)\n",
    "Qreg_s_D_min = 0          # Minumum regulated release (MCM/d)\n",
    "Qreg_s_D_max = Q_max      # Maximum regulated release (MCM/d)\n",
    "\n",
    "# Details on the initial reservoir release optimisation\n",
    "start_year = 2014\n",
    "start_month = 6\n",
    "end_year = 2016\n",
    "end_month = 9\n",
    "\n",
    "# Release optimisation options\n",
    "pop = 100\n",
    "itnum = 50000\n",
    "\n",
    "# Weight allocation for SAW (Bal, Sup, Sto) and Simple Selective methods\n",
    "weight_bal = 0.5    # Balanced weighting between supply and storage\n",
    "weight_sup = 0.4    # Weight prioritizing supply (higher value favors supply)\n",
    "weight_sto = 0.6    # Weight prioritizing storage (lower value favors storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Run the reservoir simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful\n"
     ]
    }
   ],
   "source": [
    "obsdata = pd.read_csv(path + '/data/observed_data.csv',index_col = 'Date')\n",
    "ini_date = str(start_year) + '-' + str(start_month).zfill(2) + '-01'\n",
    "s_ini = obsdata[obsdata.index == ini_date].iloc[0,0]    \n",
    "\n",
    "# Continuous simulations\n",
    "for leadtime in [2,4,6]:\n",
    "    for decision_time in [1,2]:\n",
    "        # Compute the iteration times for continuous simulation\n",
    "        total_months = int(((end_year - start_year) * 12 + (end_month - start_month) + 1)/decision_time)\n",
    "        \n",
    "        for MCDM in range(4,5):  # for each MCDM method\n",
    "            year = start_year\n",
    "            month = start_month\n",
    "            S_all = None\n",
    "\n",
    "            for _ in range(total_months):     # for the number of iterations during defined simulation period\n",
    "                # Optimize based on scenario\n",
    "                for scenario in [1,2,3,4,5]:  # run the reservoir simulation functions for each flow forecast/scenario\n",
    "                    \n",
    "                    # Determine current S_value based on previous S_all for MCDM_VM method\n",
    "                    if S_all is not None:  # if previous S_all is exist\n",
    "                        if scenario == 1:\n",
    "                            S_value = S_all['S_pf'].iloc[-1]\n",
    "                        elif scenario == 2:\n",
    "                            S_value = S_all['S_wc'].iloc[-1]\n",
    "                        elif scenario == 3:\n",
    "                            S_value = S_all['S_d20'].iloc[-1]\n",
    "                        elif scenario == 4:\n",
    "                            S_value = S_all['S_esp'].iloc[-1]\n",
    "                        elif scenario == 5:\n",
    "                            S_value = S_all['S_sffs'].iloc[-1]\n",
    "                    else:  # if previous S_all is not exist\n",
    "                        S_value = s_ini  # Ï¥àÍ∏∞Í∞í ÎòêÎäî Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©\n",
    "                    \n",
    "                    if scenario <= 3:\n",
    "                        reservoir_opt_deterministic(reservoir_name, month, year, leadtime, pop, itnum, scenario) # Deterministic\n",
    "                    else:\n",
    "                        reservoir_opt_probabilistic(reservoir_name, month, year, leadtime, pop, itnum, scenario) # Probabilistic\n",
    "\n",
    "                # When the Pareto front for release is generated, we select a single relese schedule using different MCDM methods\n",
    "                if MCDM in [1, 2, 3]:    # Simple Additive Weighting (SAW) method\n",
    "                    S_all = MCDM_SAW(MCDM, year, month, reservoir_name, decision_time, leadtime)\n",
    "                elif MCDM in [4, 5]:     # Variable Weighting method\n",
    "                    S_all = MCDM_VW(MCDM, year, month, reservoir_name, decision_time, leadtime)\n",
    "                elif MCDM in [6, 7, 8]:  # Reference Point method\n",
    "                    S_all = MCDM_RP(MCDM, year, month, reservoir_name, decision_time, leadtime)\n",
    "                else:\n",
    "                    continue  # Skip invalid MCDM values\n",
    "\n",
    "                # Update month and year\n",
    "                if month + decision_time > 12:\n",
    "                    year += 1\n",
    "                    month = month + decision_time - 12\n",
    "                else:\n",
    "                    month += decision_time\n",
    "print(\"Successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful\n"
     ]
    }
   ],
   "source": [
    "# Load observed data from a CSV file, setting 'Date' as the index\n",
    "obsdata = pd.read_csv(path + '/data/observed_data.csv', index_col='Date')\n",
    "# Define the initial date based on the start year and month, and set the initial storage value\n",
    "ini_date = str(start_year) + '-' + str(start_month).zfill(2) + '-01'\n",
    "s_ini = obsdata[obsdata.index == ini_date].iloc[0, 0]    \n",
    "\n",
    "# Start continuous simulations\n",
    "for leadtime in [2, 4, 6]:  # Different lead times for simulation\n",
    "    for decision_time in [1, 2]:  # Different decision intervals\n",
    "        # Calculate the total number of months for the continuous simulation\n",
    "        total_months = int(((end_year - start_year) * 12 + (end_month - start_month) + 1) / decision_time)\n",
    "        \n",
    "        for MCDM in range(5, 6):  # Loop through different MCDM methods (currently set to a single method)\n",
    "            year = start_year  # Initialize the simulation year\n",
    "            month = start_month  # Initialize the simulation month\n",
    "            S_all = None  # Initialize S_all to store results from each simulation round\n",
    "\n",
    "            # Iterate through each month in the simulation period\n",
    "            for _ in range(total_months):\n",
    "                # Optimize based on scenario\n",
    "                for scenario in [1, 2, 3, 4, 5]:  # Loop through different scenarios for reservoir management\n",
    "                    \n",
    "                    # Determine the current S_value based on the previous S_all for the Variable Weighting method\n",
    "                    if S_all is not None:  # If previous S_all exists\n",
    "                        if scenario == 1:\n",
    "                            S_value = S_all['S_pf'].iloc[-1]  # Get value from Pareto front for scenario 1\n",
    "                        elif scenario == 2:\n",
    "                            S_value = S_all['S_wc'].iloc[-1]  # Get value for water conservation\n",
    "                        elif scenario == 3:\n",
    "                            S_value = S_all['S_d20'].iloc[-1]  # Get value for 20-day drought scenario\n",
    "                        elif scenario == 4:\n",
    "                            S_value = S_all['S_esp'].iloc[-1]  # Get value from ESP method\n",
    "                        elif scenario == 5:\n",
    "                            S_value = S_all['S_sffs'].iloc[-1]  # Get value from SFFS method\n",
    "                    else:  # If previous S_all does not exist\n",
    "                        S_value = s_ini  # Use initial value from observed data as a fallback\n",
    "                    \n",
    "                    # Optimize reservoir operations based on scenario\n",
    "                    if scenario <= 3:\n",
    "                        reservoir_opt_deterministic(reservoir_name, month, year, leadtime, pop, itnum, scenario)  # Use deterministic optimization\n",
    "                    else:\n",
    "                        reservoir_opt_probabilistic(reservoir_name, month, year, leadtime, pop, itnum, scenario)  # Use probabilistic optimization\n",
    "\n",
    "                # After generating the Pareto front for releases, select a single release schedule using different MCDM methods\n",
    "                if MCDM in [1, 2, 3]:  # Simple Additive Weighting (SAW) method\n",
    "                    S_all = MCDM_SAW(MCDM, year, month, reservoir_name, decision_time, leadtime)\n",
    "                elif MCDM in [4, 5]:  # Variable Weighting method\n",
    "                    S_all = MCDM_VW(MCDM, year, month, reservoir_name, decision_time, leadtime, scenario)\n",
    "                elif MCDM in [6, 7, 8]:  # Reference Point method\n",
    "                    S_all = MCDM_RP(MCDM, year, month, reservoir_name, decision_time, leadtime)\n",
    "                else:\n",
    "                    continue  # Skip any invalid MCDM values\n",
    "\n",
    "                # Update month and year for the next iteration\n",
    "                if month + decision_time > 12:\n",
    "                    year += 1  # Move to the next year if the month exceeds 12\n",
    "                    month = month + decision_time - 12  # Calculate the new month\n",
    "                else:\n",
    "                    month += decision_time  # Otherwise, just increment the month\n",
    "            \n",
    "print(\"Successful\")  # Indicate that the simulation has completed successfully"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
